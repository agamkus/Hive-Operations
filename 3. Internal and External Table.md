# Internal and External Table:
The basic difference between internal and external tables is that when we create an internal table, Hive is itself responsible for the data, and it takes control of the entire life cycle of the metadata as well as the actual data itself.

In contrast, when we create an external table, Hive is responsible for taking care of only the schema of the table. It does not take care of the actual data.

When we apply the drop command on an internal table, the entire metadata available in the **metastore**, as well as the actual table data that is present in the HDFS, gets deleted.

However, when you apply the drop command on an external table, the entire metadata available in the metastore gets deleted but the actual table data available in the HDFS remains intact there.

**Note:** When we create a table in Hive, it is an internal table by default. To create an external table, you need to explicitly use the keyword **"EXTERNAL"** while writing the query to create the table.

**Usage:** Suppose a particular table is very common and is useful for other applications as well. In such a case, it is preferable to use an external table, because when you drop the table, the data would remain intact in the HDFS, and it can be used by some other applications. However, when you drop an internal table, the schema, as well as the table data, gets deleted and it becomes unavailable for other applications as well.

## Creating Internal Table:

1.	Let’s create an internal table **"user_info"** using below commands:
    
    ````
    create table if not exists user_info (
       id int,
       age int,
       gender string,
       profession string,
       reviews int
    )
    row format delimited fields terminated by '|'
    lines terminated by '\n' 
    stored as textfile;
    ````


    ![image](https://user-images.githubusercontent.com/56078504/199255566-ff94b5c4-685c-4a2f-bd40-3dffcc649d25.png)

2.	Get schema of the created table:
    
    ````
    describe user_info;
    ````
    
    ![image](https://user-images.githubusercontent.com/56078504/199255659-c7d3e02e-d45f-4b24-abf4-73dee3e71ea8.png)

**Note:** Here, we need to make sure to specify the format of data that is available in the **HDFS/local**, for instance, fields are delimited by ‘|’ and lines are terminated by ‘\n’, otherwise, you will not be able to read the data into the tables.

### Load Data into Internal Table:

To load the data, you have to specify a path from where you will load the data into these table. In our case, data is at location /home/cloudera/hive_operations/u.user. So, load data by executing below command:

````
load data local inpath '/home/cloudera/hive_operations/u.user' into table user_info;
````

![image](https://user-images.githubusercontent.com/56078504/199262102-2fe3e2f9-e228-41b1-96cb-7e07ff78f176.png)

Now check data by executing below query:

````
select * from user_info limit 5;
````

![image](https://user-images.githubusercontent.com/56078504/199292829-71f07e95-02f7-405c-8485-c32075264505.png)


## Creating External Table:

The syntax to create an external table is almost the same as that for creating an internal table; you just need to specify the keyword **"EXTERNAL"** while creating an external table. Below are the queries to create an external table **"user_info_external"** and load data into it:

    
    create external table if not exists user_info_external (
           id int,
           age int,
           gender string,
           profession string,
           reviews int
     )
    row format delimited fields terminated by '|'
    lines terminated by '\n' 
    stored as textfile;
    
    ![image](https://user-images.githubusercontent.com/56078504/200243597-9ddcc618-e2fc-4d60-acfd-3c316008f7de.png)

    
### Load Data into External Table:
Run below command to load data into table **user_info_external**:

   ````
    load data local inpath '/home/cloudera/hive_operations/u.user' into table user_info_external;
   ````
   
   ![image](https://user-images.githubusercontent.com/56078504/200243899-169133d9-415d-41bf-ba9c-f5b785efe5c5.png)


Let’s run below commands to see the tables created in HDFS:

````
hadoop fs -ls /user/hive/warehouse/ratings.db
````

![image](https://user-images.githubusercontent.com/56078504/200243969-057d0c42-ff45-4185-b013-dc9f101f3fc3.png)


We can see that there are 2 tables **user_info** and **user_info_external** have been created.

## Dropping Tables:
We can use the “drop table table_name” command to drop both the tables. Below are the queries:

````
drop table user_info;
drop table user_info_external;
````

![image](https://user-images.githubusercontent.com/56078504/200244324-e0b35ac3-7783-4e06-84c7-939c2ccec855.png)


Let’s check the storage location on HDFS again:

````
hadoop fs -ls /user/hive/warehouse/ratings.db
````

![image](https://user-images.githubusercontent.com/56078504/200244371-f8bd150d-b82c-405d-937a-1230b9a8dbaf.png)


We can see that although the internal table data has been deleted from the HDFS, the external table data is still there, even after we dropped the external table.
